---
title: "p8105_hw6_kpp2126"
author: "Kevin P. Patterson"
date: "2022-11-26"
output: github_document
---

```{r libraries}
library(tidyverse)
library(dplyr)
library(labelled)
library(patchwork)
#install.packages("ggcorrplot")
library(ggcorrplot)
#install.packages("table1")
library(table1)
#install.packages("RCurl")
library(RCurl)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
```

## Problem 1

```{r weather data}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```
The boostrap is helpful when you’d like to perform inference for a parameter / value / summary that doesn’t have an easy-to-write-down distribution in the usual repeated sampling framework. 
1. We’ll focus on a simple linear regression with tmax as the response and tmin as the predictor, and are interested in the distribution of two quantities estimated from these data:
$$
\hat{r}^2
$$
$$
log(\hat{\beta_0}*\hat{\beta_1})
$$
* Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of these two quantities.
```{r 5k bootstrap samples}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  ggplot(aes(x = r.squared)) + geom_density()
```

2. Plot the distribution of your estimates, and describe these in words. Using the 5000 bootstrap estimates, identify the 2.5% and 97.5% quantiles to provide a 95% confidence interval for r̂ 2 and log(β̂ 0∗β̂ 1)
*Note: `broom::glance()` is helpful for extracting r̂ 2 from a fitted regression, and`broom::tidy()` (with some additional wrangling) should help in computing log(β̂ 0∗β̂ 1).
```{r}
weather_df %>% 
  modelr::bootstrap(n = 1000) %>% 
  mutate(
    models = map(strap, ~lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy)) %>% 
  select(-strap, -models) %>% 
  unnest(results) %>% 
  select(id = `.id`, term, estimate) %>% 
  pivot_wider(
    names_from = term, 
    values_from = estimate) %>% 
  rename(beta0 = `(Intercept)`, beta1 = tmin) %>% 
  mutate(log_b0b1 = log(beta0 * beta1)) %>% 
  ggplot(aes(x = log_b0b1)) + geom_density()
```


## Problem 2
```{r}
x <- getURL("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv")
hom_df <- read.csv(text = x)
```

1. Create a city_state variable (e.g. “Baltimore, MD”), and a binary variable indicating whether the homicide is solved. 
*Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO – these don’t report victim race. Also omit Tulsa, AL – this is a data entry mistake. For this problem, limit your analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.
```{r}
tidyhom_df = hom_df %>%
  mutate(city_state = str_c(city, state, sep = ", "), #create city_state variable
         resolved = as.numeric(disposition == "Closed by arrest"), #create 1=resolved or 0=unresolved
         victim_age = as.numeric(victim_age),
         victim_sex = fct_relevel(victim_sex, "Female"),
         city_state = ifelse(city_state %in% c("Milwaukee, wI"), "Milwaukee, WI", city_state),
         victim_race = fct_relevel(victim_race, "White")) %>% #fixing Milwaukee, wI to WI
  filter(!city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
         victim_race %in% c("White", "Black"),
         !victim_sex %in% c("Unknown")) %>% #39,693 obs before drop_na(victim_age)
  drop_na(victim_age) %>% #39,403 obs after drop_na(victim_age) (290 less)
  select(victim_race, victim_age, victim_sex, city_state, resolved)

##this was used to double check that the NA coercion in as.numeric(victim_age) -> drop_na(victim_age)
#tidyhom_df %>%
#  select(city_state, victim_race, victim_age) %>%
#  filter(victim_age == "Unknown") %>%
#  count #290 unknown ages

##checking totals on number of age 0's
tidyhom_df %>%
  select(city_state, victim_race, victim_age) %>%
  #group_by(city_state) %>%
  filter(victim_age == "0") %>%
  count #297 homicides at age 0
```


2. For the city of Baltimore, MD, use the glm function to fit a logistic regression with (`outcome`) resolved vs unresolved as the outcome and `victim age`, `sex` and `race` as predictors.

*Save the output of glm as an R object; 
*Apply the `broom::tidy` to this object;
*Obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.
```{r baltimore}
#creating logistic Rframe
fit_logistic_baltimore = 
  tidyhom_df %>%
  filter(city_state == "Baltimore, MD") %>%
  glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial()) 

#table
fit_logistic_baltimore %>% 
  broom::tidy() %>% 
  mutate(OR = exp(estimate),
         OR.ci.low = exp(estimate - 1.96 * std.error),
         OR.ci.high = exp(estimate + 1.96 * std.error)) %>%
  select(term, log_OR = estimate, OR, OR.ci.low, OR.ci.high, p.value) %>% 
  knitr::kable(digits = 3)
```

3. Now run glm for each of the cities in your dataset;
*Extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. 
*Do this within a “tidy” pipeline, making use of `purrr::map`, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.
```{r glm for all cities}
glm_all = 
  tidyhom_df %>%
  nest(data = -city_state) %>% #need to remove to resolve issues with select later
  mutate(model = map(data, ~ glm(resolved ~ victim_age + victim_race + victim_sex, data = ., family = binomial())),
         result = map(.x=model, ~broom::tidy(.x,conf.int = TRUE))) %>% 
  select(city_state, result) %>% 
  unnest(result) %>%
  mutate(
    OR = exp(estimate),
    OR.ci.low = exp(estimate - 1.96 * std.error),
    OR.ci.high = exp(estimate + 1.96 * std.error)
  ) %>% 
  select(city_state, term, starts_with("OR")) %>% 
  filter(term == "victim_sexMale")
```

4. Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.
```{r}
glm_all %>%
  mutate(
    city_state = fct_reorder(city_state, -OR)
  ) %>%
  ggplot(aes(x = OR, y = city_state)) +
  geom_point() +
  geom_errorbar(aes(xmin = OR.ci.low , xmax = OR.ci.high)) +
  labs(title = "OR's resolved Homicide Cases based on Victim Sex across 47 Cities",
       y = "City, State",
       x = "Odds Ratio Male vs Female (95% CI)")
```


##Problem 3

1. Load and clean the data for regression analysis (i.e. convert numeric to factor where appropriate, check for missing data, etc.).
```{r load data}
##load raw dataframe
birthweight_rawdf =
  read_csv("data/birthweight.csv")

##check column NA's
birthweight_rawdf %>%
  summarise(across(everything(), ~sum(is.na(.x)))) %>%
  knitr::kable() #no NA's identified

##tidy
birthweight_df =
  birthweight_rawdf %>%
  drop_na() %>% #quality assurance check
  set_value_labels(babysex = c("male" = 1, "female" = 2), 
                   frace = c("white" = 1, "black" = 2, "asian" = 3, "puerto rican" = 4, "other" = 8, "uknown" = 9),
                   malform = c("absent" = 0, "present" = 1),
                   mrace = c("white" = 1, "black" = 2, "asian" = 3, "puerto rican" = 4, "other" = 8)) %>%
  mutate_if(is.labelled, to_factor) 
```

2. Propose a regression model for birthweight. 
*This model may be based on a hypothesized structure for the factors that underlie birthweight, on a data-driven model-building process, or a combination of the two. 

**Table 1**
```{r}
pvalue <- function(x, ...) {
    # Construct vectors of data y, and groups (strata) g
    y <- unlist(x)
    g <- factor(rep(1:length(x), times=sapply(x, length)))
    if (is.numeric(y)) {
        # For numeric variables, perform a standard 2-sample t-test
        p <- t.test(y ~ g)$p.value
    } else {
        # For categorical variables, perform a chi-squared test of independence
        p <- chisq.test(table(y, g))$p.value
    }
    # Format the p-value, using an HTML entity for the less-than sign.
    # The initial empty string places the output on the line below the variable label.
    c("", sub("<", "&lt;", format.pval(p, digits=3, eps=0.001)))
}

table1(~ bwt + bhead + blength + delwt + fincome + frace + gaweeks + malform + menarche + mheight + momage + mrace + parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain | babysex,
    data=birthweight_df, overall=F, extra.col=list(`P-value`=pvalue))
```

*Describe your modeling process and show a plot of model residuals against fitted values – use `add_predictions` and `add_residuals` in making this plot.
Based on what was significant above from Table 1, I will include unadjusted significant variables into my model that bear some crude relation to birthweight. While some variables appear to not be significant such as `smoken`, previous literature has shown positively associated relationships between maternal smoking while pregnant and low birth weight. 

```{r lm model}
bwt_fit = lm(bwt ~ bhead + blength + frace + mrace + momage + wtgain + smoken, data = birthweight_df)
summary(bwt_fit)

bwt_fit %>% 
  broom::glance() %>% 
  broom::tidy()

bwt_fit_plot = 
  birthweight_df %>% 
  add_residuals(bwt_fit) %>% 
  add_predictions(bwt_fit) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(color = "red") +
  labs(title = "Model 1: Residual vs. Fitted Estimate",
       y = "Residual",
       x = "Fitted Estimate")
```


3. Compare your model to two others:

*One using length at birth and gestational age as predictors (main effects only)
```{r}
bwt_fit2 = lm(bwt ~ blength + gaweeks, data = birthweight_df)
summary(bwt_fit2)

bwt_fit2 %>% 
  broom::glance()%>% 
  broom::tidy()

bwt_fit2_plot = 
  birthweight_df %>% 
  add_residuals(bwt_fit2) %>% 
  add_predictions(bwt_fit2) %>%
  ggplot(aes(x = pred, y = resid)) + 
  geom_point(color = "blue") +
  labs(title = "Model 2 Main Effects: Residual vs. Fitted Estimate",
       y = "Residual",
       x = "Fitted Estimate")
```

*One using head circumference, length, sex, and all interactions (including the three-way interaction) between these
```{r}
bwt_fit3 = lm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = birthweight_df)
summary(bwt_fit3)

bwt_fit3 %>% 
  broom::glance()%>% 
  broom::tidy()

bwt_fit3_plot = 
  birthweight_df %>% 
  add_residuals(bwt_fit3) %>% 
  add_predictions(bwt_fit3) %>%
  ggplot(aes(x = pred, y = resid)) + geom_point() +
  labs(title = "Model 3 Three Interaction: Residual vs. Fitted Estimate",
       y = "Residual",
       x = "Fitted Estimate")
```

**plots**
```{r}
bwt_fit_plot + bwt_fit2_plot + bwt_fit3_plot
```
These plots show in all cases mostly random spread along 0 with some outliers, more so seen in the model 3.

*Make this comparison in terms of the cross-validated prediction error; use crossv_mc and functions in purrr as appropriate.

```{r cross validation}
#cross val setup
cv_birthweight_df = 
  crossv_mc(birthweight_df, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
  )

#map function
cv_birthweight_df = 
  cv_birthweight_df %>% 
  mutate(
    bwt_fit  = map(train, ~lm(bwt ~ bhead + blength + frace + mrace + momage + wtgain + smoken, data = .x)),
    bwt_fit2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
    bwt_fit3  = map(train, ~lm(bwt ~ bhead + blength + babysex + bhead*blength*babysex, data = .x))) %>% 
  mutate(
    rmse_fit = map2_dbl(bwt_fit, test, ~rmse(model = .x, data = .y)),
    rmse_fit2 = map2_dbl(bwt_fit2, test, ~rmse(model = .x, data = .y)),
    rmse_fit3 = map2_dbl(bwt_fit3, test, ~rmse(model = .x, data = .y)))

#plot
cv_birthweight_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse, color = model)) + 
  geom_violin() +
  labs(title = "Violin Plot: 3 Models with RMSE",
       y = "RMSE estimate",
       x = "Model")
```

The `bwt_fit` model and the `bwt_fit3` are the best fit models with the `fit` model being the better of the two because of their smaller rmse values. the second model has a larger rmse and may not be as comprehensive in relevant predictors as the first fit and fit3 have accounted for more predictors and interactions.